  package    com . salesforce . webdev . sitecrawler ;   import   java . util . ArrayList ;  import   java . util . Arrays ;  import   java . util . Collection ;  import   java . util . Collections ;  import   java . util . LinkedList ;  import   java . util . List ;  import   java . util . Set ;  import    java . util . concurrent . ConcurrentSkipListSet ;  import    java . util . concurrent . LinkedBlockingDeque ;  import    java . util . concurrent . TimeUnit ;  import     java . util . concurrent . atomic . AtomicInteger ;  import      org . apache . commons . lang3 . math . NumberUtils ;  import   org . slf4j . Logger ;  import   org . slf4j . LoggerFactory ;  import   org . slf4j . MDC ;  import    com . gargoylesoftware . htmlunit . WebClient ;  import     com . gargoylesoftware . htmlunit . util . Cookie ;  import      com . salesforce . webdev . sitecrawler . beans . CrawlProgress ;  import      com . salesforce . webdev . sitecrawler . beans . CrawlerConfiguration ;  import      com . salesforce . webdev . sitecrawler . navigation . NavigateThread ;  import      com . salesforce . webdev . sitecrawler . scheduler . LocalScheduler ;  import      com . salesforce . webdev . sitecrawler . scheduler . Organizer ;  import      com . salesforce . webdev . sitecrawler . scheduler . Scheduler ;  import      com . salesforce . webdev . sitecrawler . utils . URLCleaner ;   public class SiteCrawler  {   private final Logger  logger =  LoggerFactory . getLogger  (  getClass  ( ) ) ;   private final String  baseUrl ;   private final String  baseUrlSecure ;   private final  List  <  ? extends SiteCrawlerAction >  actions ;   private  Collection  < String >  allowedSuffixes =  new  ArrayList  < String >  ( ) ;   private boolean  requireAllowedSuffixes = true ;   private  Set  < String >  visited =  new  ConcurrentSkipListSet  < String >  ( ) ;   private AtomicInteger  visitedCounter =  new AtomicInteger  ( ) ;   private  LinkedBlockingDeque  < String >  toVisit =  new  LinkedBlockingDeque  < String >  ( ) ;   private  Collection  < String >  blocked =  new  ConcurrentSkipListSet  < String >  ( ) ;   private  Collection  < String >  allowed =  new  ConcurrentSkipListSet  < String >  ( ) ;   private  int  threadLimit =   Runtime . getRuntime  ( ) . availableProcessors  ( ) ;   private Scheduler  scheduler =  new LocalScheduler  ( this ) ;   private Organizer  organizer =  new Organizer  ( scheduler ) ;   private  double  downloadVsProcessRatio = 2 ;   private  double  maxProcessWaitingRatio = 0.4 ;   private  int  maxProcessWaiting ;   private final  int  reportProgressPerDownloadedPages = 2000 ;   private AtomicInteger  linksScheduled =  new AtomicInteger  ( ) ;   private AtomicInteger  pagesScheduled =  new AtomicInteger  ( ) ;   private AtomicInteger  actuallyVisited =  new AtomicInteger  ( ) ;   private AtomicInteger  fullyProcessed =  new AtomicInteger  ( ) ;   private boolean  running = false ;   private volatile boolean  continueProcessing = true ;   private boolean  discoverUrls = true ;   private  int  shortCircuitAfter = 0 ;   private  int  visitLogged =  - 1 ;   private boolean  disableRedirects = false ;   private boolean  enabledJavascript = false ;   private  List  < Cookie >  cookies =  new  LinkedList  < Cookie >  ( ) ;   private boolean  forcePause = false ;   public SiteCrawler  (  String baseUrl ,  String baseUrlSecure ,  SiteCrawlerAction ...  actions )  {  this  ( baseUrl , baseUrlSecure ,  Collections . unmodifiableList  (  Arrays . asList  ( actions ) ) ) ; }   public SiteCrawler  (  String baseUrl ,  String baseUrlSecure ,   List  <  ? extends SiteCrawlerAction > actions )  {    this . baseUrl = baseUrl ;    this . baseUrlSecure = baseUrlSecure ;  if  (  null == actions )  {    this . actions =  Collections . emptyList  ( ) ; } else  {    this . actions = actions ; }   parseVMOptions  ( ) ;   addDefaultAllowedSuffixes  ( ) ; }   public void setId  (  String id )  {   scheduler . setId  ( id ) ; }   public Logger getLogger  ( )  {  return logger ; }   public void setThreadLimit  (   int threadLimit )  {  if  (  threadLimit < 1 )  {  throw  new IllegalArgumentException  ( "Cannot have less the 1 thread" ) ; }    this . threadLimit = threadLimit ;  if  ( running )  {   reset  ( ) ; } }   public  int getThreadLimit  ( )  {  return threadLimit ; }   public void setMaxProcessWaitingRatio  (   double maxProcessWaitingRatio )  {  if  (   maxProcessWaitingRatio <= 0 ||  maxProcessWaitingRatio > 1 )  {  throw  new IllegalArgumentException  ( "maxProcessWaitingRatio has to be between 0 and 1" ) ; }    this . maxProcessWaitingRatio = maxProcessWaitingRatio ;  if  ( running )  {   reset  ( ) ; } }   public  double getMaxProcessWaitingRatio  ( )  {  return maxProcessWaitingRatio ; }   public void setDownloadVsProcessRatio  (   double downloadVsProcessRatio )  {  if  (   downloadVsProcessRatio < 0 ||  downloadVsProcessRatio > 1 )  {  throw  new IllegalArgumentException  ( "maxProcessWaitingRatio has to be between 0 and 1" ) ; }    this . downloadVsProcessRatio = downloadVsProcessRatio ;  if  ( running )  {   reset  ( ) ; } }   public  double getDownloadVsProcessRatio  ( )  {  return downloadVsProcessRatio ; }   public void pause  ( )  {   forcePause = true ; }   public void unpause  ( )  {   forcePause = false ; }   public boolean isPaused  ( )  {  return forcePause ; }   public void hardPause  ( )  {   pause  ( ) ;   scheduler . pause  ( ) ;   shutdown  ( ) ; }   public void hardUnpause  ( )  {    this . continueProcessing = true ;   init  ( ) ;   scheduler . unpause  ( ) ;   unpause  ( ) ; }   public boolean getContinueProcessing  ( )  {  return continueProcessing ; }   public void setIncludePath  (   Collection  < String > paths )  {   logger . debug  ( "Setting include path with {} items (currently scheduled: {})" ,  paths . size  ( ) ,  toVisit . size  ( ) ) ;  for ( String path : paths )  {   offerUrl  ( path ) ; }   logger . debug  ( "DONE Setting include path, currently scheduled: {})" ,  toVisit . size  ( ) ) ; }   public void offerUrl  (  String url )  {  String  excludePath =  prependBaseUrlIfNeeded  ( url ) ;  boolean  ex =  isExcluded  ( excludePath ) ;  boolean  sc =  isScheduled  ( url ) ;  if  (   ! ex &&  ! sc )  {   toVisit . add  ( url ) ; } }   public void setMaxProcessWaiting  (   int maxProcessWaiting )  {  if  (  maxProcessWaiting < 1 )  {  throw  new IllegalArgumentException  ( "maxProcessWaiting cannot be less then 1" ) ; }    this . maxProcessWaiting = maxProcessWaiting ; }   public  int getMaxProcessWaiting  ( )  {  return  this . maxProcessWaiting ; }   public  int getShortCircuitAfter  ( )  {  return shortCircuitAfter ; }   public void setShortCircuitAfter  (   int shortCircuitAfter )  {    this . shortCircuitAfter = shortCircuitAfter ; }   public void disableCrawling  ( )  {   discoverUrls = false ; }   public void enableRedirects  ( )  {    this . disableRedirects = false ; }   public void disableRedirects  ( )  {    this . disableRedirects = true ; }   public void enableJavaScript  ( )  {    this . enabledJavascript = true ; }   public void disableJavaScript  ( )  {    this . enabledJavascript = false ; }   public void setRequireAllowedSuffixes  (  boolean requireAllowedSuffixes )  {    this . requireAllowedSuffixes = requireAllowedSuffixes ; }   private URLCleaner  urlCleaner =  new URLCleaner  ( ) ;   public URLCleaner getUrlCleaner  ( )  {  return urlCleaner ; }   public void addCookie  (  String name ,  String value ,  String domain )  {   addCookie  (  new Cookie  ( domain , name , value ) ) ; }   public void addCookie  (  Cookie cookie )  {   cookies . add  ( cookie ) ; }   public boolean clearCookies  ( )  {   cookies . clear  ( ) ;  return true ; }   public  List  < Cookie > getCookies  ( )  {  return cookies ; }   public void setBlocked  (   Collection  < String > blocked )  {  if  (  null == blocked )  {  return ; }  for ( String block : blocked )  {    this . blocked . add  ( block ) ; } }   public void setAllowed  (   Collection  < String > allowed )  {  if  (  null == allowed )  {  return ; }  for ( String allow : allowed )  {    this . allowed . add  ( allow ) ; } }   public  Collection  < String > getAllowedSuffixes  ( )  {  return allowedSuffixes ; }   public void addAllowedSuffixes  (   Collection  < String > allowed )  {    this . allowedSuffixes . addAll  ( allowed ) ; }   public void navigate  ( )  {  if  (  toVisit . isEmpty  ( ) )  {  if  (  null != baseUrl )  {   toVisit . add  ( baseUrl ) ; } else  if  (  null != baseUrlSecure )  {   toVisit . add  ( baseUrlSecure ) ; } }   Object  [ ]  args =  {  toVisit . size  ( ) ,  actions . size  ( ) , actions } ;   
<<<<<<<
scheduler
=======
logger
>>>>>>>
 . 
<<<<<<<
unpause
=======
info
>>>>>>>
  ( "Starting crawl with the {} defined endpoints and {} plugins: {}" , args ) ;    this . running = true ;   init  ( ) ;   startCrawler  ( ) ;   scheduler . pause  ( ) ;   shutdown  ( ) ; }   public void incrementVisited  ( )  {   actuallyVisited . getAndIncrement  ( ) ; }   public  int getPagesScheduled  ( )  {  return  pagesScheduled . get  ( ) ; }   public void incrementScheduled  ( )  {   pagesScheduled . getAndIncrement  ( ) ; }   public void decrementScheduled  ( )  {   pagesScheduled . getAndDecrement  ( ) ; }   public  int getLinksScheduled  ( )  {  return  linksScheduled . get  ( ) ; }   public void decrementLinksScheduled  ( )  {   linksScheduled . getAndDecrement  ( ) ; }   public void incrementFullyProcessed  ( )  {   fullyProcessed . getAndIncrement  ( ) ; }   public void shutdown  ( )  {    this . continueProcessing = false ;   scheduler . shutdown  ( ) ; }   public String getCrawlProgress  ( )  {  CrawlProgress  progress =  getCrawlProgressBean  ( ) ;  StringBuilder  sb =  new StringBuilder  ( ) ;    sb . append  (  progress . crawled ) . append  ( " crawled. " ) ;    sb . append  (  progress . leftToCrawl ) . append  ( " left to crawl. " ) ;    sb . append  (  progress . scheduledForDownload ) . append  ( " scheduled for download. " ) ;    sb . append  (  progress . scheduledForProcessing ) . append  ( " scheduled for processing. " ) ;    sb . append  (  progress . fullyProcessed ) . append  ( " fully processed. " ) ;    sb . append  (  progress . complete ) . append  ( "% complete." ) ;  return  sb . toString  ( ) ; }   public CrawlProgress getCrawlProgressBean  ( )  {   int  leftToCrawl =    toVisit . size  ( ) +  linksScheduled . get  ( ) - threadLimit ;  CrawlProgress  crawlProgress =  new CrawlProgress  ( ) ;    crawlProgress . crawled =  actuallyVisited . get  ( ) ;    crawlProgress . leftToCrawl = leftToCrawl ;    crawlProgress . scheduledForDownload =  linksScheduled . get  ( ) ;    crawlProgress . scheduledForProcessing =  pagesScheduled . get  ( ) ;    crawlProgress . fullyProcessed =  fullyProcessed . get  ( ) ;    crawlProgress . complete =   Math . round  (   (   new Double  (  fullyProcessed . get  ( ) ) /  (   fullyProcessed . get  ( ) + leftToCrawl ) ) * 10000 ) / 100.0 ;  return crawlProgress ; }   public CrawlerConfiguration getCrawlerConfiguration  ( )  {  CrawlerConfiguration  crawlerConfiguration =  new CrawlerConfiguration  ( ) ;    crawlerConfiguration . baseUrl = baseUrl ;    crawlerConfiguration . baseUrlSecure = baseUrlSecure ;    crawlerConfiguration . threadLimit = threadLimit ;    crawlerConfiguration . downloadVsProcessRatio = downloadVsProcessRatio ;    crawlerConfiguration . maxProcessWaitingRatio = maxProcessWaitingRatio ;    crawlerConfiguration . maxProcessWaiting = maxProcessWaiting ;    crawlerConfiguration . shortCircuitAfter = shortCircuitAfter ;    crawlerConfiguration . disableRedirects = disableRedirects ;    crawlerConfiguration . enabledJavascript = enabledJavascript ;    crawlerConfiguration . actions = actions ;  return crawlerConfiguration ; }   private void init  ( )  {   scheduler . init  ( ) ; }   private void reset  ( )  {   hardPause  ( ) ;   hardUnpause  ( ) ; }   private void parseVMOptions  ( )  {   int  threadLimit =  NumberUtils . toInt  (  System . getProperty  ( "sc:threadLimit" ) ) ;  if  (  threadLimit > 0 )  {   setThreadLimit  ( threadLimit ) ; }   int  maxProcessWaiting =  NumberUtils . toInt  (  System . getProperty  ( "sc:maxProcessWaiting" ) ) ;  if  (  maxProcessWaiting > 0 )  {   setMaxProcessWaiting  ( maxProcessWaiting ) ; }   int  shortCircuitAfter =  NumberUtils . toInt  (  System . getProperty  ( "sc:shortCircuitAfter" ) ) ;  if  (  shortCircuitAfter > 0 )  {   setShortCircuitAfter  ( shortCircuitAfter ) ; }   int  downloadVsProcessRatio =  NumberUtils . toInt  (  System . getProperty  ( "sc:downloadVsProcessRatio" ) ) ;  if  (  downloadVsProcessRatio > 0 )  {   setDownloadVsProcessRatio  ( downloadVsProcessRatio ) ; } }   private void addDefaultAllowedSuffixes  ( )  {   allowedSuffixes . add  ( "/" ) ;   allowedSuffixes . add  ( ".jsp" ) ;   allowedSuffixes . add  ( ".htm" ) ;   allowedSuffixes . add  ( ".html" ) ; }   private void startCrawler  ( )  {  while  (  shouldContinueCrawling  ( ) )  {   updateCrawlProgress  ( ) ;  String  url ;  try  {   url =  toVisit . poll  ( 5 ,  TimeUnit . SECONDS ) ;  if  (  null == url )  {  continue ; }   url =  prependBaseUrlIfNeeded  ( url ) ; }  catch (   InterruptedException e )  {   logger . error  ( "We were interrupted waiting for the next link, exiting..." , e ) ;    Thread . currentThread  ( ) . interrupt  ( ) ;  return ; }  if  (  isExcluded  ( url ) )  {   logger . trace  ( "This URL is excluded: {}" , url ) ;  continue ; }   organizer . submitUrl  ( url ) ;   linksScheduled . getAndIncrement  ( ) ;   visited . add  ( url ) ;  String  cleanUrl =  urlCleaner . getCleanedUrl  ( url ) ;  if  (  null != cleanUrl )  {   visited . add  ( cleanUrl ) ; }   visitedCounter . getAndIncrement  ( ) ; }   logger . info  ( "Done crawling, {} links visited. (crosscheck: {})" ,  visitedCounter . get  ( ) ,  actuallyVisited . get  ( ) ) ; }   private boolean shouldContinueCrawling  ( )  {  boolean  morePagesToVisit =     toVisit . size  ( ) > 0 ||   linksScheduled . get  ( ) > 0 ||   pagesScheduled . get  ( ) > 0 ;  if  (  ! morePagesToVisit )  {   logger . info  ( "No more pages to visit, all pages processed. Stopping this crawl for that reason." ) ;  return false ; }  if  (  ! discoverUrls )  {   logger . info  ( "discoverUrls was set to false. Stopping this crawl for that reason." ) ;  return false ; }   logger . trace  ( "Current shortcicruit setting: {}, visitedCounter: {}" , shortCircuitAfter ,  visitedCounter . get  ( ) ) ;  if  (   shortCircuitAfter != 0 &&   visitedCounter . get  ( ) > shortCircuitAfter )  {   logger . info  ( "A shortcircuit was set (at {}) and has been triggered after {} visited pages. Stopping this crawl for that reason." , shortCircuitAfter ,  visitedCounter . get  ( ) ) ;   logger . warn  (   "If you see a shortcircuit message (this one) in a production environment/build, " + "it is likely that somebody forgot to remove a debug \".setShortCircuit\" call. " + "Please report this if found." ) ;  return false ; }  if  (   ! forcePause &&  ! continueProcessing )  {   logger . info  ( "This crawler has been shutdown (without pause or reset). Thereforce, stopping the crawl" ) ;  return false ; }  return true ; }   private void updateCrawlProgress  ( )  {   int  visited =  actuallyVisited . get  ( ) ;  if  (   (    (  visited - visitLogged ) > reportProgressPerDownloadedPages &&  visited > visitLogged ) ||  visitLogged ==  - 1 )  {   logger . info  (  getCrawlProgress  ( ) ) ;   visitLogged = visited ; } }   private String prependBaseUrlIfNeeded  (  String url )  {  if  (  null == url )  {  throw  new NullPointerException  ( "url cannot be null" ) ; }  if  (  url . contains  ( "://" ) )  {  return url ; }  if  (  !  url . startsWith  ( "/" ) )  {   url =  "/" . concat  ( url ) ; }  if  (  null != baseUrlSecure )  {  return  baseUrlSecure . concat  ( url ) ; }  if  (  null != baseUrl )  {  return  baseUrl . concat  ( url ) ; }  throw  new NullPointerException  ( "Cannot have both baseUrl AND baseUrlSecure be null!" ) ; }   public boolean isExcluded  (  String url )  {  boolean  startsWithBaseUrl = false ;  boolean  startsWithBaseUrlSecure = false ;  boolean  allGood = false ;  if  (   null != baseUrl &&  url . startsWith  ( baseUrl ) )  {   logger . trace  ( "[isExcluded] startsWithBaseUrl: {}" , url ) ;   startsWithBaseUrl = true ; }  if  (   null != baseUrlSecure &&  url . startsWith  ( baseUrlSecure ) )  {   logger . trace  ( "[isExcluded] startsWith baseUrlSecure: {}" , url ) ;   startsWithBaseUrlSecure = true ; }  if  (     url . length  ( ) > 1 &&  url . startsWith  ( "/" ) &&  !  url . startsWith  ( "//" ) )  {   logger . trace  ( "[isExcluded] This is a relative url, pointing to the BASE: {}" , url ) ;   allGood = true ; }  if  (    ! startsWithBaseUrl &&  ! startsWithBaseUrlSecure &&  ! allGood )  {   logger . trace  ( "[isExcluded] !startsWithBaseUrl && !startsWithBaseUrlSecure && !allGood: {}" , url ) ;  return true ; }  boolean  hasAllowedSuffix = false ;  String  suffix =    url . split  ( "\\?" ) [ 0 ] . toLowerCase  ( ) ;  for ( String allowedSuffix : allowedSuffixes )  {   logger . trace  ( "[isExcluded] Matching allowed suffix [{}] against URL {}" , allowedSuffix , suffix , url ) ;  if  (  suffix . endsWith  ( allowedSuffix ) )  {   hasAllowedSuffix = true ;  break ; } }  if  (  ! requireAllowedSuffixes )  {   logger . trace  ( "[isExcluded] requireAllowedSuffixes = false, so setting hasAllowedSuffix to true" ) ;   hasAllowedSuffix = true ; }  if  (  ! hasAllowedSuffix )  {   logger . trace  ( "[isExcluded] not allowing suffix {} for {}" , suffix , url ) ;  return true ; }  if  (  visited . contains  ( url ) )  {   logger . trace  ( "[isExcluded] We already visited [{}], skipping it." , url ) ;  return true ; }  if  (  listContainsSubstring  ( blocked , url ) )  {   logger . trace  ( "[isExcluded] This URL is blocked [{}], skipping it." , url ) ;  return true ; }  if  (   !  allowed . isEmpty  ( ) &&  !  listContainsSubstring  ( allowed , url ) )  {   logger . trace  ( "[isExcluded] This URL is not allowed [{}], skipping it." , url ) ;  return true ; }  String  cleanUrl =  urlCleaner . getCleanedUrl  ( url ) ;  if  (   null != cleanUrl &&  visited . contains  ( cleanUrl ) )  {   logger . trace  ( "[isExcluded] The cleaned URL is blocked [{}], skipping it." , url ) ;  return true ; }  return false ; }   public boolean isScheduled  (  String url )  {  if  (  toVisit . contains  ( url ) )  {  return true ; }  return false ; }   private boolean listContainsSubstring  (   Collection  < String > list ,  String checkStr )  {  for ( String s : list )  {   logger . trace  ( "CHECKING This URL [{}] for {}" , checkStr , s ) ;  if  (  checkStr . contains  ( s ) )  {   logger . trace  ( "This URL [{}] matches because of {}, so we're returning true." , checkStr , s ) ;  return true ; } }   logger . trace  ( "This URL [{}] did NOT match anything., checked against collection of size: {}" , checkStr ,  list . size  ( ) ) ;  return false ; } }