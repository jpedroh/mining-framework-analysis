  package    uk . co . flax . luwak ;   import   java . io . Closeable ;  import   java . io . IOException ;  import  java . util .  * ;  import    java . util . concurrent . Executors ;  import    java . util . concurrent . ScheduledExecutorService ;  import      org . apache . lucene . analysis . core . KeywordAnalyzer ;  import    org . apache . lucene . document .  * ;  import    org . apache . lucene . index .  * ;  import    org . apache . lucene . search .  * ;  import      org . apache . lucene . search . spans . SpanCollector ;  import      uk . co . flax . luwak . util . RewriteException ;  import      uk . co . flax . luwak . util . SpanExtractor ;  import      uk . co . flax . luwak . util . SpanRewriter ;  import     org . apache . lucene . store . Directory ;  import     org . apache . lucene . store . RAMDirectory ;  import     org . apache . lucene . util . BytesRef ;  import     org . apache . lucene . util . BytesRefBuilder ;  import      uk . co . flax . luwak . presearcher . PresearcherMatches ;  import      uk . co . flax . luwak . util . ForceNoBulkScoringQuery ;   public class Monitor  implements  Closeable  {   protected final MonitorQueryParser  queryParser ;   protected final Presearcher  presearcher ;   protected final QueryDecomposer  decomposer ;   private final QueryIndex  queryIndex ;   private final  List  < QueryIndexUpdateListener >  listeners =  new  ArrayList  < >  ( ) ;   protected  long  slowLogLimit = 2000000 ;   private final  long  commitBatchSize ;   private final boolean  storeQueries ;   public static final class FIELDS  {   public static final String  id = "_id" ;   public static final String  del = "_del" ;   public static final String  hash = "_hash" ;   public static final String  mq = "_mq" ; }   private final ScheduledExecutorService  purgeExecutor ;   private  long  lastPurged =  - 1 ;   public Monitor  (  MonitorQueryParser queryParser ,  Presearcher presearcher ,  IndexWriter indexWriter ,  QueryIndexConfiguration configuration )  throws IOException  {    this . queryParser = queryParser ;    this . presearcher = presearcher ;    this . decomposer =  configuration . getQueryDecomposer  ( ) ;    this . queryIndex =  new QueryIndex  ( indexWriter ) ;    this . storeQueries =  configuration . storeQueries  ( ) ;   prepareQueryCache  (  this . storeQueries ) ;   long  purgeFrequency =  configuration . getPurgeFrequency  ( ) ;    this . purgeExecutor =  Executors . newSingleThreadScheduledExecutor  ( ) ;    this . purgeExecutor . scheduleAtFixedRate  (   ( ) ->  {  try  {   purgeCache  ( ) ; }  catch (   Throwable e )  {   afterPurgeError  ( e ) ; } } , purgeFrequency , purgeFrequency ,  configuration . getPurgeFrequencyUnits  ( ) ) ;    this . commitBatchSize =  configuration . getQueryUpdateBufferSize  ( ) ; }   public Monitor  (  MonitorQueryParser queryParser ,  Presearcher presearcher )  throws IOException  {  this  ( queryParser , presearcher ,  defaultIndexWriter  (  new RAMDirectory  ( ) ) ,  new QueryIndexConfiguration  ( ) ) ; }   public Monitor  (  MonitorQueryParser queryParser ,  Presearcher presearcher ,  QueryIndexConfiguration config )  throws IOException  {  this  ( queryParser , presearcher ,  defaultIndexWriter  (  new RAMDirectory  ( ) ) , config ) ; }   public Monitor  (  MonitorQueryParser queryParser ,  Presearcher presearcher ,  Directory directory )  throws IOException  {  this  ( queryParser , presearcher ,  defaultIndexWriter  ( directory ) ,  new QueryIndexConfiguration  ( ) ) ; }   public Monitor  (  MonitorQueryParser queryParser ,  Presearcher presearcher ,  Directory directory ,  QueryIndexConfiguration config )  throws IOException  {  this  ( queryParser , presearcher ,  defaultIndexWriter  ( directory ) , config ) ; }   public Monitor  (  MonitorQueryParser queryParser ,  Presearcher presearcher ,  IndexWriter indexWriter )  throws IOException  {  this  ( queryParser , presearcher , indexWriter ,  new QueryIndexConfiguration  ( ) ) ; }   static IndexWriter defaultIndexWriter  (  Directory directory )  throws IOException  {  IndexWriterConfig  iwc =  new IndexWriterConfig  (  new KeywordAnalyzer  ( ) ) ;  TieredMergePolicy  mergePolicy =  new TieredMergePolicy  ( ) ;   mergePolicy . setSegmentsPerTier  ( 4 ) ;   iwc . setMergePolicy  ( mergePolicy ) ;   iwc . setOpenMode  (   IndexWriterConfig . OpenMode . CREATE_OR_APPEND ) ;  return  new IndexWriter  ( directory , iwc ) ; }   public void addQueryIndexUpdateListener  (  QueryIndexUpdateListener listener )  {   listeners . add  ( listener ) ; }   public QueryCacheStats getQueryCacheStats  ( )  {  return  new QueryCacheStats  (  queryIndex . numDocs  ( ) ,  queryIndex . cacheSize  ( ) , lastPurged ) ; }   public static class QueryCacheStats  {   public final  int  queries ;   public final  int  cachedQueries ;   public final  long  lastPurged ;   public QueryCacheStats  (   int queries ,   int cachedQueries ,   long lastPurged )  {    this . queries = queries ;    this . cachedQueries = cachedQueries ;    this . lastPurged = lastPurged ; } }   private void prepareQueryCache  (  boolean storeQueries )  throws IOException  {  if  (  storeQueries == false )  {   clear  ( ) ;  return ; }   final  List  < Exception >  parseErrors =  new  LinkedList  < >  ( ) ;   final  Set  < BytesRef >  seenHashes =  new  HashSet  < >  ( ) ;   final  Set  < String >  seenIds =  new  HashSet  < >  ( ) ;   queryIndex . purgeCache  (  newCache ->  queryIndex . scan  (   ( id , query , dataValues ) ->  {  if  (  seenIds . contains  ( id ) )  {  return ; }   seenIds . add  ( id ) ;  BytesRef  serializedMQ =   dataValues . mq . get  (  dataValues . doc ) ;  MonitorQuery  mq =  MonitorQuery . deserialize  ( serializedMQ ) ;  BytesRef  hash =  mq . hash  ( ) ;  if  (  seenHashes . contains  ( hash ) )  {  return ; }   seenHashes . add  ( hash ) ;  try  {  for ( QueryCacheEntry ce :  decomposeQuery  ( mq ) )  {   newCache . put  (  ce . hash , ce ) ; } }  catch (   Exception e )  {   parseErrors . add  ( e ) ; } } ) ) ;  if  (   parseErrors . size  ( ) != 0 )  throw  new IOException  (  "Error populating cache - some queries couldn't be parsed:" + parseErrors ) ; }   private void commit  (   List  < Indexable > updates )  throws IOException  {   beforeCommit  ( updates ) ;   queryIndex . commit  ( updates ) ;   afterCommit  ( updates ) ; }   private void afterPurge  ( )  {  for ( QueryIndexUpdateListener listener : listeners )  {   listener . onPurge  ( ) ; } }   private void afterPurgeError  (  Throwable t )  {  for ( QueryIndexUpdateListener listener : listeners )  {   listener . onPurgeError  ( t ) ; } }   private void beforeCommit  (   List  < Indexable > updates )  {  if  (  updates == null )  {  for ( QueryIndexUpdateListener listener : listeners )  {   listener . beforeDelete  ( ) ; } } else  {  for ( QueryIndexUpdateListener listener : listeners )  {   listener . beforeUpdate  ( updates ) ; } } }   private void afterCommit  (   List  < Indexable > updates )  {  if  (  updates == null )  {  for ( QueryIndexUpdateListener listener : listeners )  {   listener . afterDelete  ( ) ; } } else  {  for ( QueryIndexUpdateListener listener : listeners )  {   listener . afterUpdate  ( updates ) ; } } }   public void purgeCache  ( )  throws IOException  {   queryIndex . purgeCache  (  newCache ->  queryIndex . scan  (   ( id , query , dataValues ) ->  {  if  (  query != null )   newCache . put  (  BytesRef . deepCopyOf  (  query . hash ) , query ) ; } ) ) ;   lastPurged =  System . nanoTime  ( ) ;   afterPurge  ( ) ; }   public void setSlowLogLimit  (   long limit )  {    this . slowLogLimit = limit ; }    @ Override public void close  ( )  throws IOException  {   purgeExecutor . shutdown  ( ) ;   queryIndex . closeWhileHandlingException  ( ) ; }   public void update  (   Iterable  < MonitorQuery > queries )  throws IOException , UpdateException  {   List  < QueryError >  errors =  new  ArrayList  < >  ( ) ;   List  < Indexable >  updates =  new  ArrayList  < >  ( ) ;  for ( MonitorQuery query : queries )  {  try  {  for ( QueryCacheEntry queryCacheEntry :  decomposeQuery  ( query ) )  {   updates . add  (  new Indexable  (  query . getId  ( ) , queryCacheEntry ,  buildIndexableQuery  (  query . getId  ( ) , query , queryCacheEntry ) ) ) ; } }  catch (   Exception e )  {   errors . add  (  new QueryError  ( query , e ) ) ; }  if  (   updates . size  ( ) > commitBatchSize )  {   commit  ( updates ) ;   updates . clear  ( ) ; } }   commit  ( updates ) ;  if  (   errors . isEmpty  ( ) == false )  throw  new UpdateException  ( errors ) ; }   private  Iterable  < QueryCacheEntry > decomposeQuery  (  MonitorQuery query )  throws Exception  {  Query  q =  queryParser . parse  (  query . getQuery  ( ) ,  query . getMetadata  ( ) ) ;  BytesRef  rootHash =  query . hash  ( ) ;   int  upto = 0 ;   List  < QueryCacheEntry >  cacheEntries =  new  LinkedList  < >  ( ) ;  for ( Query subquery :  decomposer . decompose  ( q ) )  {  BytesRefBuilder  subHash =  new BytesRefBuilder  ( ) ;   subHash . append  ( rootHash ) ;   subHash . append  (  new BytesRef  (  "_" +  upto ++ ) ) ;   cacheEntries . add  (  new QueryCacheEntry  (  subHash . toBytesRef  ( ) , subquery ,  query . getMetadata  ( ) ) ) ; }  return cacheEntries ; }   public void update  (  MonitorQuery ...  queries )  throws IOException , UpdateException  {   update  (  Arrays . asList  ( queries ) ) ; }   public void delete  (   Iterable  < MonitorQuery > queries )  throws IOException  {  for ( MonitorQuery mq : queries )  {   queryIndex . deleteDocuments  (  new Term  (   Monitor . FIELDS . del ,  mq . getId  ( ) ) ) ; }   commit  ( null ) ; }   public void deleteById  (   Iterable  < String > queryIds )  throws IOException  {  for ( String queryId : queryIds )  {   queryIndex . deleteDocuments  (  new Term  (  FIELDS . del , queryId ) ) ; }   commit  ( null ) ; }   public void deleteById  (  String ...  queryIds )  throws IOException  {   deleteById  (  Arrays . asList  ( queryIds ) ) ; }   public void clear  ( )  throws IOException  {   queryIndex . deleteDocuments  (  new MatchAllDocsQuery  ( ) ) ;   commit  ( null ) ; }   public  <  T  extends QueryMatch >  Matches  < T > match  (  DocumentBatch docs ,   MatcherFactory  < T > factory )  throws IOException  {   CandidateMatcher  < T >  matcher =  factory . createMatcher  ( docs ) ;   matcher . setSlowLogLimit  ( slowLogLimit ) ;   match  ( matcher ) ;  return  matcher . getMatches  ( ) ; }   public  <  T  extends QueryMatch >  Matches  < T > match  (  InputDocument doc ,   MatcherFactory  < T > factory )  throws IOException  {  return  match  (  DocumentBatch . of  ( doc ) , factory ) ; }   private class PresearcherQueryBuilder  implements   QueryIndex . QueryBuilder  {   final LeafReader  batchIndexReader ;   private PresearcherQueryBuilder  (  LeafReader batchIndexReader )  {    this . batchIndexReader = batchIndexReader ; }    @ Override public Query buildQuery  (  QueryTermFilter termFilter )  throws IOException  {  return  presearcher . buildQuery  ( batchIndexReader , termFilter ) ; } }   private  <  T  extends QueryMatch > void match  (   CandidateMatcher  < T > matcher )  throws IOException  {   StandardQueryCollector  < T >  collector =  new  StandardQueryCollector  < >  ( matcher ) ;   long  buildTime =  queryIndex . search  (  new PresearcherQueryBuilder  (  matcher . getIndexReader  ( ) ) , collector ) ;   matcher . finish  ( buildTime ,  collector . queryCount ) ; }   public MonitorQuery getQuery  (   final String queryId )  throws IOException  {  if  (  storeQueries == false )  throw  new IllegalStateException  ( "Cannot call getQuery() as queries are not stored" ) ;   final  MonitorQuery  [ ]  queryHolder =  new MonitorQuery  [ ]  { null } ;   queryIndex . search  (  new TermQuery  (  new Term  (  FIELDS . id , queryId ) ) ,   ( id , query , dataValues ) ->  {  BytesRef  serializedMQ =   dataValues . mq . get  (  dataValues . doc ) ;    queryHolder [ 0 ] =  MonitorQuery . deserialize  ( serializedMQ ) ; } ) ;  return  queryHolder [ 0 ] ; }   public  int getDisjunctCount  ( )  {  return  queryIndex . numDocs  ( ) ; }   public  int getQueryCount  ( )  throws IOException  {  return   getQueryIds  ( ) . size  ( ) ; }   public  Set  < String > getQueryIds  ( )  throws IOException  {   final  Set  < String >  ids =  new  HashSet  < >  ( ) ;   queryIndex . scan  (   ( id , query , dataValues ) ->  ids . add  ( id ) ) ;  return ids ; }   protected Document buildIndexableQuery  (  String id ,  MonitorQuery mq ,  QueryCacheEntry query )  {  Document  doc =  presearcher . indexQuery  (  query . matchQuery ,  mq . getMetadata  ( ) ) ;   doc . add  (  new StringField  (  FIELDS . id , id ,   Field . Store . NO ) ) ;   doc . add  (  new StringField  (  FIELDS . del , id ,   Field . Store . NO ) ) ;   doc . add  (  new SortedDocValuesField  (  FIELDS . id ,  new BytesRef  ( id ) ) ) ;   doc . add  (  new BinaryDocValuesField  (  FIELDS . hash ,  query . hash ) ) ;  if  ( storeQueries )   doc . add  (  new BinaryDocValuesField  (  FIELDS . mq ,  MonitorQuery . serialize  ( mq ) ) ) ;  return doc ; }   private static class StandardQueryCollector  <  T  extends QueryMatch >  implements   QueryIndex . QueryCollector  {   final  CandidateMatcher  < T >  matcher ;   int  queryCount = 0 ;   private StandardQueryCollector  (   CandidateMatcher  < T > matcher )  {    this . matcher = matcher ; }    @ Override public void matchQuery  (  String id ,  QueryCacheEntry query ,   QueryIndex . DataValues dataValues )  throws IOException  {  if  (  query == null )  return ;  try  {   queryCount ++ ;   matcher . matchQuery  ( id ,  query . matchQuery ,  query . metadata ) ; }  catch (   Exception e )  {   matcher . reportError  (  new MatchError  ( id , e ) ) ; } } }   public  <  T  extends QueryMatch >  PresearcherMatches  < T > debug  (   final DocumentBatch docs ,   MatcherFactory  < T > factory )  throws IOException  {   PresearcherQueryCollector  < T >  collector =  new  PresearcherQueryCollector  < >  (  factory . createMatcher  ( docs ) ) ;   QueryIndex . QueryBuilder  queryBuilder =  new PresearcherQueryBuilder  (  docs . getIndexReader  ( ) )  {    @ Override public Query buildQuery  (  QueryTermFilter termFilter )  throws IOException  {  try  {  return  new ForceNoBulkScoringQuery  (   SpanRewriter . INSTANCE . rewrite  (  super . buildQuery  ( termFilter ) , null ) ) ; }  catch (   RewriteException e )  {  throw  new IOException  ( e ) ; } } } ;   queryIndex . search  ( queryBuilder , collector ) ;  return  collector . getMatches  ( ) ; }   public  <  T  extends QueryMatch >  PresearcherMatches  < T > debug  (  InputDocument doc ,   MatcherFactory  < T > factory )  throws IOException  {  return  debug  (  DocumentBatch . of  ( doc ) , factory ) ; }   private class PresearcherQueryCollector  <  T  extends QueryMatch >  extends  StandardQueryCollector  < T >  {   public final  Map  < String , StringBuilder >  matchingTerms =  new  HashMap  < >  ( ) ;   private PresearcherQueryCollector  (   CandidateMatcher  < T > matcher )  {  super  ( matcher ) ; }   public  PresearcherMatches  < T > getMatches  ( )  {  return  new  PresearcherMatches  < >  ( matchingTerms ,  matcher . getMatches  ( ) ) ; }    @ Override public void matchQuery  (   final String id ,  QueryCacheEntry query ,   QueryIndex . DataValues dataValues )  throws IOException  {  SpanCollector  collector =  new SpanCollector  ( )  {    @ Override public void collectLeaf  (  PostingsEnum postingsEnum ,   int position ,  Term term )  throws IOException  {       matchingTerms . computeIfAbsent  ( id ,  i ->  new StringBuilder  ( ) ) . append  ( " " ) . append  (  term . field  ( ) ) . append  ( ":" ) . append  (   term . bytes  ( ) . utf8ToString  ( ) ) ; }    @ Override public void reset  ( )  { } } ;   SpanExtractor . collect  (  dataValues . scorer , collector , false ) ;   super . matchQuery  ( id , query , dataValues ) ; }    @ Override public boolean needsScores  ( )  {  return true ; } } }